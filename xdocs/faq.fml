<?xml version="1.0" encoding="UTF-8"?>
<faqs title="Frequently Asked Questions">

  <part id="general">
    <title>General</title>


    <faq id="heritrix">
      <question>
            What does "Heritrix" mean?
      </question>
      <answer>
        <p><em>Heritrix</em> (sometimes spelled <em>heretrix</em>) is an archaic word for <em>inheritess</em>. Since our crawler seeks to collect the digital artifacts of our culture and <i>preserve</i> them for the benefit of future researchers and generations, this name seemed apt.</p>
      </answer>
    </faq>

    <faq id="introduction">
      <question>
      Where can I go to get a good introduction/overview of Heritrix?
      </question>
      <answer>
        <p>Please see the documentation on the <a href="http://webteam.archive.org/confluence/display/Heritrix/Home">Heritrix Wiki</a>.   
        The document <a href="Mohr-et-al-2004.pdf">An Introduction to Heritrix</a> describes the Heritrix architecture.
        </p>
      </answer>
    </faq>

    <faq id="user-heritrix">
      <question>
    I need to crawl/archive a set of websites, can I use Heritrix?
      </question>
      <answer>
        <p>Yes.  Start by checking out the 
        <a href="articles/user_manual/index.html">Heritrix User Manual</a>.
        </p>
      </answer>
    </faq>

    <faq id="developer">
      <question>
    I'm a developer, can I help?
      </question>
      <answer>
        <p>Yes -- especially if you have experience with Java,
        open-source projects, and web protocols! Drop us a message or join
        our <a href="mail-lists.html">mailing lists</a> for more details.
        See also the <a href="articles/developer_manual/index.html">Heritrix
        Developer Manual</a>.
        </p>
      </answer>
    </faq>
    <faq id="license">
      <question>What license does Heritrix use?
      </question>
      <answer>
        <p>The <a href="license.html">GNU LESSER GENERAL PUBLIC LICENSE</a>.
	For discussion of 3rd party applications making use of LGPL code,
	see David Turner on 
	<a href="http://www.gnu.org/licenses/lgpl-java.html">The LGPL
	and Java</a>.</p>
      </answer>
    </faq>
  </part>

  <part id="problems">
    <title>Common Problems</title>
    <faq id="arc_closed">
      <question>How do I know when heritrix is done with an ARC file?</question>
      <answer>
      ARCs that are currently in use will have a '.open' suffix.
      Those without such a suffix are fair-game for copying.
      Also see 
      <a href="https://sourceforge.net/tracker/?func=detail&amp;aid=988277&amp;group_id=73833&amp;atid=539102">[988277] [Need feedback] "Done with ARC file" event</a> for a description for 
      how to enable logging of opening and closing of arcs.  See also
      the conf/hertrix.properties for how to enable console logging going to
      a FileHandler as well as to heritrix_out.log.</answer>
    </faq>
    <faq id="limitations">
      <question>Are there known limitations?</question>
      <answer>See the 
      <a href="articles/releasenotes/index.html">Release Notes</a>
      page.</answer>
    </faq>
    <faq id="testsfail">
      <question>Why do unit tests fail when I build?</question>
      <answer>You're probably on a platform other than linux (or using 
      a 2.6.x kernel and a JVM that is other than the release version of 
      the SUN 1.5 jdk).
      See sections 4.1.3/4.1.4 in 
      <a href="articles/releasenotes/1_0_0.html#984390">Release Notes</a>
      page.</answer>
    </faq>

    <faq id="linuxes">
    <question>Which Linux distribution should I use to run Heritrix and which kernel version do I need?</question>
    <answer><p>Heritrix does not depend on a specific Linux distribution to
    function and should work on any distro as long as a suitable Java Virtual
    Machine can be installed on it. We know that Heritrix has been successfully
    deployed on Red Hat 7.2, recent fedora core versions (2 and 4), as well as
    on suse 9.3. Heritrix is known to work well with kernel versions 2.4.x. 
    With kernel versions 2.6.x there are issues when using JVMs other then the
    release version of the SUN 1.5 jdk. See 
    <a href="#testsfail">Why do unit tests fail when I build?</a> 
    below. There are also issues when using the linux NPTL threading model, 
    particularly with older glibcs (i.e. debian).  See
    <a href="#glibc2_3_2">Glibc 2.3.2 and NPTL</a> in the release
    notes.</p></answer>
    </faq>

    <faq id="windows">
        <question>How do I run Heritrix on windows.</question>
        <answer>Before you begin, Heritrix is not supported on windows -- See
        <a href="requirements.html">requirements</a> -- mostly because we don't
        have the resources to support any more than the linux platform we use
        internally at the Internet Archive.  That said, a <code>CMD</code>
        script has been added to $HERITRIX_HOME/bin lately that does most of
        what the $HERITRIX_HOME/bin/heritrix does loading jars in the right
        order, etc. (Originally contributed by Eric Jensen 
        and finished by Max Sch√∂fmann). Max has also written up this page:
        <a href="http://www.cip.ifi.lmu.de/~schoefma/howto/run_heritrix_on_windows/">Web
        crawling: Using Heritrix on Windows</a>.
Also see <a href="http://groups.yahoo.com/group/archive-crawler/message/3019">Crawler Stalling on Windows</a>, <a 
    href="http://groups.yahoo.com/group/archive-crawler/message/2085">2085</a>
        and the items below that pertain to windows:
        <a href="#windowsstart">dns</a> and <a  href="#windowsmkdir">mkdir</a>.
        </answer>
    </faq>

    <faq id="windowsstart">
      <question>The crawler gets dns fine but nothing subsequently.
      Why?</question>
      <answer>If you are running on windows, it may be because the 
      ordering of jars on the classpath is wrong.  See
      <a href="http://groups.yahoo.com/group/archive-crawler/message/772">Why crawler [sic] nothing ???</a>.</answer>
    </faq>
    <faq id="windowsmkdir">
      <question>The crawler, running on windows, complains it cannot
      <code>mkdir</code>. Why?</question>
      <answer>
      See <a 
      href="http://groups.yahoo.com/group/archive-crawler/message/1880"></a>1880</answer>
    </faq>

    <faq id="midfetch">
      <question>I only want to download <code>text/html</code> and nothing else.  Can I do it?</question>
      <answer>Tom Emerson describes one technique here, 
      <a href="http://www.dreamersrealm.net/~tree/blog/?s=text%2Fhtml&amp;submit=GO">Focusing on HTML</a>.  
      You can also add a filter that excludes all filters that end
      in other than 'html|htm', etc., or, if you want to instead
      look at document mimetypes, you can Add a 
      <code>ContentTypeRegExpFilter</code>  filter
      as a <code>midfetch</code> filter to the http fetcher.
      This filter will be checked after the response headers
      have been downloaded but before the response content
      is fetched.  Configure it to only allow through documents of the
      Content-Type desired.  Apply the same filter at the
      writer stage of processing to eliminate recording of
      response headers in ARCs.  See the
      <a href="articles/user_manual/config.html#midfetch">User Manual</a>
      (Prerequisite URLs by-pass the midfetch filters so it is not possible
      to filter out robots.txt using this mechanism).
      </answer>
    </faq>

    <faq id="crawllogstatuscodes">
      <question>Where do I go to learn about these cryptic crawl.log status
      codes (-6, -7, -9998, etc.)?</question>
      <answer>See the <a href="http://crawler.archive.org/articles/user_manual/glossary.html#statuscodes">User Manual Glossary</a>.</answer>
    </faq>


    <faq id="toomanyopenfiles">
      <question>Why do I get
      <i>java.io.FileNotFoundException...(Too many open files)</i> or
      <i>java.io.IOException...(Too many open files)</i>?
      </question>
      <answer>
      <p>On linux, a usual upper bound is 1024 file descriptors per
      process.  To change this upper bound, there's a couple of things
      you can do.</p>  
      <p>If running the crawler as non-root (recommended), 
      you can configure limits in <code>/etc/security/limits.conf</code>. For
      example you can setup open files limit for all users in webcrawler group
      as:
<source>
# Each line describes a limit for a user in the form:
#
# domain    type    item    value
#
@webcrawler     hard    nofile  32768
</source>
      </p>
      <p>Otherwise, running as root (You need to be root to up ulimits),
      you can do the following:
    <code># (ulimit -n 4096; JAVA_OPTS=-Xmx320 bin/heritrix -p 9876)</code>
        to up the ulimit for the heritrix process only.
      </p>
      <p>Below is a rough accounting of FDs used in heritrix 1.0.x.</p>
      <p>In Heritrix, the number of concurrent threads is configurable.  The
      default frontier implementation allocates a thread per server.  Per
      server, the frontier keeps a disk-backed queue.  Disk-backed queues
      maintain three backing files with '.qin', '.qout', and '.top' suffixes
      (One to read from while the other is being written to as well as queue
      head file).  So, per thread there will be at least three
      file descriptors occupied when queues need to persist to disk.</p>
      <p>Apart from the above per thread FD cost, there is 
      a set FD cost instantiating the crawler:
      <ul>
      <li>The JVM, its native shared libs and jars count for about 40
      FDs.</li>
      <li>There are about 20 or so heritrix jars and 2 webapps.
      </li>
      <li>There are about 10-20 heritrix logging files counting
      counting lock files.</li>
      <li>Open ARC files.</li>
      <li>Miscellaneous sockets, /dev/null, /dev/random, 
      and stderr/stdout make for 10 or 20 more FDs.</li>
          </ul>
      </p>
      </answer>
      </faq>
      <faq id="oome_broadcrawl">
        <question>Why
        do I get an OutOfMemoryException ten minutes after starting 
        a broad scoped crawl?</question>
        <answer>
        <p>If using 64-bit JVM, see Gordon's note to the list on
        12/19/2005, <a href="http://groups.yahoo.com/group/archive-crawler/message/2450">Re: Large crawl experience (like, 500M links)</a>.
        </p>
        <p>See the note in
        <a hef="https://sourceforge.net/tracker/?func=detail&amp;atid=539102&amp;aid=896772&amp;group_id=73833">[ 896772 ] "Site-first"/'frontline' prioritization</a> and this Release Note, <a href="http://crawler.archive.org/articles/releasenotes.html#1_0_0_limitations">5.1.1 Crawl Size Upper Bounds</a>.
        See this note by Kris from the list, <a href="http://groups.yahoo.com/group/archive-crawler/message/1027">1027</a> for how
        to mitigate memory-use when using HostQueuesFrontier. The advice is
        less applicable if using a post-1.2.0, BdbFrontier Heritrix.  See
        sections 'Crawl Size Upper Bounds Update' in the Release Notes.
        </p>
        </answer>
      </faq>
      <faq id="new_writer">
        <question>Can I insert
        the crawl download directly into a MYSQL database instead of
        into an ARC file on disk while crawling?</question>
        <answer>
        <p>Yes.  See <a href="http://groups.yahoo.com/group/archive-crawler/message/508">RE: [archive-crawler] Inserting information to MYSQL during crawl</a>
        for pointers on how but also see the rest of this thread for why you
            might rather do database insertion post-crawl rather than during.
        </p>
        </answer>
      </faq>
      <faq id="mirror">
        <question>Does Heritrix have to write ARC files?</question>
        <answer>
        <p>See MirrorWriterProcessor.  It writes a file per
        URL to the filesystem using a name that is a derivative of the
        requested URL.
        </p>
        </answer>
      </faq>


      <faq id="eclipse_assert">
        <question>Why when
        running heritrix in eclipse does it complain about the
        'assert' keyword?</question>
        <answer>
        <p>You'll need to configure Eclipse for Java 5.0 compliance to get rid
        of the assert errors (prior to Java 5.0 'assert' was not a keyword and 
        currently Eclipse defaults 1.3).  This can be done by going into
        "Window>Preferences>Java/Compiler>Compliance and Classfiles" and
        setting "Compiler compliance level" to 5.0.  Make sure the
        'Use default compliance level' is UNCHECKED and that the
        'Generated .class files compatibility' and 'Source compatibility'
        are also set to 5.0.</p>
        </answer>
      </faq>
      <faq id="crawl_finished">
        <question>Why won't my crawl finish?</question>
        <answer>
        <p>The crawl can get hung up on sites that are actually down or are non-responsive.
        Manual intervention is necessary in such cases. 
        Study the frontier to get a picture of what is left to be crawled.
        Looking at the local errors log will give let you see the problems with currently
        crawled URIs.  Along with robots.txt retries, you will probably also see
        httpclient timeouts.
        In general you want to look for repetition of problems with particular
        host/URIs.</p> 


        <p>Grepping the local errors log is a bit tricky because
        of the shape of its content. Its recommend that you first "flatten"
        the local errors file.  Here's an example :
        <source>% cat  local-errors.log | tr -d \\\n | perl -pe 's/([0-9]{17} )/\n$1/g'</source>
        </p>

         <p>This will remove all new lines and then add a new line in front of 17-digit dates (hopefully only 17-digit tokens followed by a space are dates.).  The result is one line per entry with a 17-digit
         date prefix. Makes it easier to parse.
         </p>

        <p>To eliminate URIs for unresponsive hosts from the frontier queue,
        pause the crawl and block the
        fetch from that host by creating a new per-host setting 
        -- an override -- in the preselector processor.</p>

         <p>Also, check for any hung threads. This does not happen
         anymore (0.8.0+). Check the threads report for threads that
         have been active for a long time but that should not be: 
         i.e. documents being downloaded are small in size.
        </p>
        <p>Once you've identified hung threads, kill and replace it.</p>

        </answer>
      </faq>
      <faq id="traps">
        <question>What are crawler traps?</question>
        <answer>
        <p>Traps are infinite page sources put up to occupy ('trap') a crawler.
        Traps may be as innocent as a calendar that returns pages years into
        the future or not-so-innocent 
        <a href="http://spiders.must.die.net/">http://spiders.must.die.net/</a>.
        Traps are created by CGIs/server-side code that dynamically conjures
        'nonsense' pages or else exploits combination of soft and relative links
        to generate URI paths of infinite variety and depth.
        Once identified, use filters to guard against falling in.
        </p>

        <p>Another trap that works by feeding documents of infinite sizes
        to the crawler is
        http://yahoo.domain.com.au/tools/spiderbait.aspx* as in
        http://yahoo.domain.com.au/tools/spiderbait.aspx?state=vic or
        http://yahoo.domain.com.au/tools/spiderbait.aspx?state=nsw.
        To filter out infinite document size traps, add a maximum doc.
        size filter to your crawl order.
        </p>
        <p>See <a href="#crawl_junk">What to do when I'm crawling "junk"?</a>
        </p>
        </answer>
      </faq>

      <faq id="crawl_junk">
        <question>What do I do to avoid crawling "junk"?</question>
        <answer>
         <p>In the past crawls were stopped when we ran into "junk."  
         An example of what we mean by "junk" is the crawler stuck
         in a web calender crawling the year 2020.  Nowadays, if 
         "junk" is detected, we'll pause the crawl and set filters
         to eliminate "junk" and then resume (Eliminated URIs will
         show in the logs.  Helps when doing post-crawl analysis).
         </p>
         <p>To help guard against the crawling of "junk" setup 
         the pathological and path-depth filters.
         This will also help the crawler avoid <a href="#traps">traps</a>.
         Recommended values for pathological filter is 3 repetitions of same
         pattern -- e.g. /images/images/images/... -- and for path-depth, a
         value of 20.
         </p>

        </answer>
      </faq>

      <faq id="war">
        <question>Can Heritrix be made run in Tomcat (or Websphere, or
        Resin, or Weblogic)?  Does it have to be run embedded in
        Jetty?</question>
        <answer>
        <p>Try out Heritrix bundled as a WAR file.
        Use the maven 'war' target to produce a heritrix.war or
        pull the war from the build 
        <a href="http://builds.archive.org:8080/cruisecontrol/buildresults/HEAD-heritrix">downloads page</a> (Click on the 'Build Artifacts' link).
        Heritrix as a WAR is available in HEAD only (post-1.2.0) and
        currently has 'experimental' status (i.e. It needs exercising).
        </p>
        </answer>
      </faq>
      <faq id="embedding">
        <question>Can I embedd Heritrix in another application?
        </question>
        <answer>
        <p>Sure.  Make sure all that is in the Heritrix lib directory is on
        your CLASSPATH (ensuring the heritrix.jar is found first).
        Thereafter, using HEAD (post-1.2.0), doing the following should
        get you a long ways: 
        <source>Heritrix h = new Heritrix();
        h.launch();</source>
        You'll then need to have your program hangaround while the crawl runs.
        See
        <a href="http://groups.yahoo.com/group/archive-crawler/message/1276">message1276</a>  for an example. See also the answer to the next question
        and this page up on our wiki, <a
        href="http://crawler.archive.org/cgi-bin/wiki.pl?EmbeddingHeritrix">Embedding Heritrix</a>.
        </p>
        </answer>
      </faq>
      <faq id="cmdlinecontrol">
        <question>Can I stop/pause and get status from a running Heritrix
        using command-line tools? Can I remote control Heritrix?
        </question>
        <answer>
        <p>A JMX interface has been added to the crawler.  The intent is that
        all features of the UI are exposed in JMX so Heritrix can be
        remotely controlled.</p>
        <p>A cmdline control utility that makes use of the JMX API has been
        added.  The script can
        be found in the scripts directory.  Its
packaged as a jar file named <code>cmdline-jmxclient.X.X.X.jar</code>.  
It has no dependencies on other jars being found in its classpath so it 
can be safely moved from this location.  Its only dependency is jdk1.5.0.
To learn more, obtain client usage by typing the following:
<code>${PATH_TO_JDK1.5.0}/bin/java -jar cmdline-jmxclient.X.X.X.jar</code>. 
See also <a 
href="http://crawler.archive.org/cmdline-jmxclient/">cmdline-jmxclient</a>
to learn more.</p>  
        </answer>
      </faq>

      <faq id="more_than_one_job">
        <question>What techniques exist for crawling more than one job at 
        time?
        </question>
        <answer>
        <p>See this, <a href="http://groups.yahoo.com/group/archive-crawler/message/1182">1182</a>, Tom Emerson note for a suggestion.</p>  
        <p>Its also possible post-1.4.0 to run multiple Heritrix instances
        in a single JVM.  Browse to <code>/local-instances.jsp</code>.
        </p>
        </answer>
      </faq>


<faq id="toethreads">
<question>Why are the main crawler worker threads called "ToeThreads"??
</question>
<answer>
<p>While the mascots of web crawlers have usually been spider-related,
I'd rather think of Heritrix as a centipede or millipede: fast and
many-segmented.</p>
<p>Anything that "crawls" over many things at once would presumably have
a lot of feet and toes. Heritrix will often use many hundreds of worker
threads to "crawl", but 'WorkerThread' or 'CrawlThread' seem mundane.</p>
<p>So instead, we have 'ToeThreads'.  :)</p>
</answer>
</faq>

<faq id="using_heritrix">
<question>Who is using Heritrix?</question>
<answer>
<p>Below is listing of users of Heritrix (To qualify for inclusion in the
list below, send a description of a couple of lines to the mailing list).
<ul>
    <li><a href="http://www.bok.hi.is/">The National and University Library of
    Iceland</a>: Crawls the
    entire <i>.is</i> domain (~11,000 domains) using Heritrix. Has performed
    complete snapshot using Heritrix 1.0.4 (35million URIs) and plans on
    running three more snapshots in 2005. See <a href="http://groups.yahoo.com/group/archive-crawler/message/1385">1385</a>.</li>
    <li><a href="http://www.lib.helsinki.fi/english/index.htm">The National
    Library of Finland</a>:  Has used Heritrix to crawl Finnish
    museum sites and sites pertaining to the June 2004 European parliament
    elections.  The main crawl done in
    2004 was of Finnish university sites (~4million URLs).
    Kaisa supplies more detail on how this larger crawl was done: <a 
    href="http://groups.yahoo.com/group/archive-crawler/message/1406">1406</a>.
    </li>
    <li><a href="http://www.geometa.info">metainfo</a>: 
    Geometa.info is a search machine for spatially related geo-data,
    geo-services and geo-news for Switzerland, Germany and Austria. We use
    Heritrix with specialised plugins to find geo-relevant datas and
    websites. This are formats like Geotiff-, GML-, Interlis-, ESRI-files,
    WFS- or WMS-services and other sites with georelevant content.
    Geometabot (Heritrix) is the feeder for the Lucene search engine which
    provides the coresearchservice for geometa.info.
    </li>
    <li>Saurabh Pathak and Donna Bergmark have written a module for Heritrix
    that asks of a Rainbow classifier if a page should be crawled or not.
    See <a 
    href="http://groups.yahoo.com/group/archive-crawler/message/1905">1905</a>
    for their announcement of the project with links to src and
    HOWTO documentation.
    </li>
</ul>
</p>
</answer>
</faq>
      <faq id="nutchwax">
        <question>I've downloaded all these ARC files, now what?
        </question>
        <answer>
        <p>See the <ulink 
        url="http://crawler.archive.org/articles/developer_manual.html#arcs">Developer's Manual</ulink> for more on ARCs and tools for reading and writing
        them.  There are also tools for searching ARC collections
        available over in the <ulink 
        url="http://archive-access.sourceforge.net/">archive-access</ulink>
        project.  Checkout the nutch-based <ulink 
        url="http://archive-access.sourceforge.net/projects/nutch/">NutchWAX</ulink>
        and its companion viewer application, <ulink 
        url="http://archive-access.sourceforge.net/projects/nutch/">WERA</ulink>.
        </p>
        </answer>
      </faq>

  </part>

  <part id="references">
    <title>References</title>

    <faq id="archive_data">
      <question>Where can I learn more about what is stored at 
      the Internet Archive, the ARC file format, and tools for manipulating
      ARC files?
      </question>
      <answer>
        <p>See the ARC section in the
        <a href="articles/developer_manual.html#arcs">Developer
        Manual</a>.
        </p>
      </answer>
    </faq>
   
    <faq id="where_refs">
      <question>Where can I get more background on Heritrix and 
        learn more about "crawling" in general?
      </question>
      <answer>
        <p>The following are all worth at least a quick skim:
    <ol>
    <li><a href="http://en.wikipedia.org/wiki/Web_crawler">The Wikipedia
    Webcrawler</a> page offers a nice introduction on general crawling
    problem.  It has a good overview of current, most cited literature.</li>
    <li><a href="http://citeseer.nj.nec.com/heydon99mercator.html">Mercator: A 
    Scalable, Extensible Web Crawler</a> is an overview of the original
    Mercator design, which the Heritrix crawler parallels in many ways.</li>
    <li><a href="http://citeseer.nj.nec.com/najork01highperformance.html">High-performance Web Crawling</a> is info on experience scaling Mercator.</li>

    <li><a 
    href="http://citeseer.nj.nec.com/heydon00performance.html">Performance 
    Limitations of the Java Core Libraries</a> is info on Mercator's
    experience working around Java problems and bottlenecks. 
    Fortunately, many of these issues have been improved for us by later JVMs
    and Java core API updates -- but some of these are still issues, and in
    any case it gives a good flavor for the kinds of
    problems and profiling one might need to do.
    </li>
    <li><a href="http://vigna.dsi.unimi.it/ftp/papers/UbiCrawler.pdf">Ubicrawler</a>, a scalable distributed
    web crawler.
    </li>
    <li><a href="http://xldb.fc.ul.pt/daniel/vnRT.pdf">The Viuva Negra crawler</a> paper describes
    common architectures and common issues encountered crawling as introduction to the VN crawler.
    The paper ends with comparison of the various crawlers from the literature.
    </li>

    <li><a href="http://citeseer.nj.nec.com/leung01towards.html">Towards
    Web-Scale Web Archeaology</a> is a higher-level view, not as focused on
    crawling details, but rather the post-crawl needs that motivate crawling
    in the first place.</li>
   <li>A number of other potentially interesting papers are linked off
   the "crawl-links.html" file in the <a 
    href="http://groups.yahoo.com/group/archive-crawler/files/">YahooGroups
        files section...</a>
    </li>
    <li><a href="http://groups.yahoo.com/group/archive-crawler/message/1498">Msg1498</a> is a note from the list on page similarity/containment issues.
    </li>

    <li>Thesis paper on creation specialized Frontier and other modules
    for Heritrix by Kristinn Sigurdsson:
    <a href="http://vefsofnun.bok.hi.is/thesis/ar.pdf">Adaptive Revisiting
    with Heritrix</a></li>
    </ol>
    </p>
      </answer>
    </faq>
  </part>  
</faqs>
